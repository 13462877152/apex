

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>apex.parallel.distributed &mdash; Apex 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="../../../index.html" class="icon icon-home"> Apex
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">AMP:  Automatic Mixed Precision</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">apex.amp</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../amp.html#opt-levels-and-properties"><code class="docutils literal notranslate"><span class="pre">opt_level</span></code>s and Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../amp.html#properties">Properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../amp.html#opt-levels"><code class="docutils literal notranslate"><span class="pre">opt_level</span></code>s</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../amp.html#o0-fp32-training"><code class="docutils literal notranslate"><span class="pre">O0</span></code>:  FP32 training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../amp.html#o1-mixed-precision-recommended-for-typical-use"><code class="docutils literal notranslate"><span class="pre">O1</span></code>:  Mixed Precision (recommended for typical use)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../amp.html#o2-almost-fp16-mixed-precision"><code class="docutils literal notranslate"><span class="pre">O2</span></code>:  “Almost FP16” Mixed Precision</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../amp.html#o3-fp16-training"><code class="docutils literal notranslate"><span class="pre">O3</span></code>:  FP16 training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../amp.html#module-apex.amp">Unified API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../amp.html#checkpointing">Checkpointing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../amp.html#advanced-use-cases">Advanced use cases</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../advanced.html">Advanced Amp Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../advanced.html#gans">GANs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../advanced.html#gradient-clipping">Gradient clipping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../advanced.html#custom-user-defined-autograd-functions">Custom/user-defined autograd functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../advanced.html#forcing-particular-layers-functions-to-a-desired-type">Forcing particular layers/functions to a desired type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../advanced.html#multiple-models-optimizers-losses">Multiple models/optimizers/losses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../advanced.html#gradient-accumulation-across-iterations">Gradient accumulation across iterations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../advanced.html#custom-data-batch-types">Custom data batch types</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../amp.html#transition-guide-for-old-api-users">Transition guide for old API users</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../amp.html#for-users-of-the-old-amp-api">For users of the old “Amp” API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../amp.html#for-users-of-the-old-fp16-optimizer">For users of the old FP16_Optimizer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../parallel.html">apex.parallel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../parallel.html#utility-functions">Utility functions</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Fused Optimizers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../optimizers.html">apex.optimizers</a></li>
</ul>
<p class="caption"><span class="caption-text">Fused Layer Norm</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../layernorm.html">apex.normalization.fused_layer_norm</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Apex</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../parallel.html">apex.parallel</a> &raquo;</li>
        
      <li>apex.parallel.distributed</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for apex.parallel.distributed</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules</span> <span class="k">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">chain</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">importlib</span>
<span class="kn">from</span> <span class="nn">..multi_tensor_apply</span> <span class="k">import</span> <span class="n">multi_tensor_applier</span>

<span class="n">imported_flatten_impl</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">def</span> <span class="nf">import_flatten_impl</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">flatten_impl</span><span class="p">,</span> <span class="n">unflatten_impl</span><span class="p">,</span> <span class="n">imported_flatten_impl</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">apex_C</span>
        <span class="n">flatten_impl</span> <span class="o">=</span> <span class="n">apex_C</span><span class="o">.</span><span class="n">flatten</span>
        <span class="n">unflatten_impl</span> <span class="o">=</span> <span class="n">apex_C</span><span class="o">.</span><span class="n">unflatten</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.&quot;</span><span class="p">)</span>
        <span class="n">flatten_impl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_flatten_dense_tensors</span>
        <span class="n">unflatten_impl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_unflatten_dense_tensors</span>
    <span class="n">imported_flatten_impl</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">bucket</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">imported_flatten_impl</span><span class="p">:</span>
        <span class="n">import_flatten_impl</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">flatten_impl</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="n">coalesced</span><span class="p">,</span> <span class="n">bucket</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">imported_flatten_impl</span><span class="p">:</span>
        <span class="n">import_flatten_impl</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">unflatten_impl</span><span class="p">(</span><span class="n">coalesced</span><span class="p">,</span> <span class="n">bucket</span><span class="p">)</span>

<span class="c1"># apply_dist_call requires that tensors in &#39;bucket&#39; are all the same type.</span>
<span class="k">def</span> <span class="nf">apply_flat_dist_call</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">call</span><span class="p">,</span> <span class="n">extra_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="n">coalesced</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">extra_args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">call</span><span class="p">(</span><span class="n">coalesced</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">call</span><span class="p">(</span><span class="n">coalesced</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">call</span> <span class="ow">is</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">:</span>
        <span class="n">coalesced</span> <span class="o">/=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">buf</span><span class="p">,</span> <span class="n">synced</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">coalesced</span><span class="p">,</span> <span class="n">bucket</span><span class="p">)):</span>
        <span class="n">buf</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">synced</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">split_half_float_double</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
    <span class="n">dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;torch.cuda.HalfTensor&quot;</span><span class="p">,</span>  <span class="s2">&quot;torch.cuda.FloatTensor&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.cuda.DoubleTensor&quot;</span><span class="p">]</span>
    <span class="n">buckets</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dtypes</span><span class="p">):</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">dtype</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">bucket</span><span class="p">:</span>
            <span class="n">buckets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buckets</span>

<span class="k">def</span> <span class="nf">split_by_type</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
    <span class="n">buckets</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">tp</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">buckets</span><span class="p">:</span>
            <span class="n">buckets</span><span class="p">[</span><span class="n">tp</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">buckets</span><span class="p">[</span><span class="n">tp</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buckets</span>

<span class="c1"># flat_dist_call organizes &#39;tensors&#39; by type.</span>
<span class="k">def</span> <span class="nf">flat_dist_call</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">call</span><span class="p">,</span> <span class="n">extra_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">buckets</span> <span class="o">=</span> <span class="n">split_by_type</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">tp</span> <span class="ow">in</span> <span class="n">buckets</span><span class="p">:</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="n">buckets</span><span class="p">[</span><span class="n">tp</span><span class="p">]</span>
        <span class="n">apply_flat_dist_call</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">call</span><span class="p">,</span> <span class="n">extra_args</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">extract_tensors</span><span class="p">(</span><span class="n">maybe_tensor</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">maybe_tensor</span><span class="p">):</span>
        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">maybe_tensor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">maybe_tensor</span><span class="p">:</span>
                <span class="n">extract_tensors</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">return</span>


<div class="viewcode-block" id="Reducer"><a class="viewcode-back" href="../../../parallel.html#apex.parallel.Reducer">[docs]</a><span class="k">class</span> <span class="nc">Reducer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :class:`apex.parallel.Reducer` is a simple class that helps allreduce a module&#39;s parameters</span>
<span class="sd">    across processes.  :class:`Reducer` is intended to give the user additional control:</span>
<span class="sd">    Unlike :class:`DistributedDataParallel`, :class:`Reducer` will not automatically allreduce</span>
<span class="sd">    parameters during ``backward()``.</span>
<span class="sd">    Instead, :class:`Reducer` waits for the user to call ``&lt;reducer_instance&gt;.reduce()`` manually.</span>
<span class="sd">    This enables, for example, delaying the allreduce to be carried out every</span>
<span class="sd">    several iterations instead of every single iteration.</span>

<span class="sd">    Like :class:`DistributedDataParallel`, :class:`Reducer` averages any tensors it allreduces</span>
<span class="sd">    over the number of participating processes.</span>

<span class="sd">    :class:`Reducer` is designed to work with the upstream launch utility script</span>
<span class="sd">    ``torch.distributed.launch`` with ``--nproc_per_node &lt;= number of gpus per node``.</span>
<span class="sd">    When used with this launcher, :class:`Reducer` assumes 1:1 mapping of processes to GPUs.</span>
<span class="sd">    It also assumes that your script calls ``torch.cuda.set_device(args.rank)`` before creating the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        module_or_grads_list: Either a network definition (module) being run in multi-gpu/distributed mode, or an iterable of gradients to be reduced.  If a module is passed in, the Reducer constructor will sync the parameters across processes (broadcasting from rank 0) to make sure they&#39;re all initialized with the same values.  If a list of gradients (that came from some module) is passed in, the user is responsible for manually syncing that module&#39;s parameters at the beginning of training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_or_grads_list</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module_or_grads_list</span><span class="p">,</span> <span class="n">Module</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module_or_grads_list</span>
            <span class="n">flat_dist_call</span><span class="p">([</span><span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()],</span> <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">extract_tensors</span><span class="p">(</span><span class="n">module_or_grads_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">flat_dist_call</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">flat_dist_call</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grads</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistributedDataParallel"><a class="viewcode-back" href="../../../parallel.html#apex.parallel.DistributedDataParallel">[docs]</a><span class="k">class</span> <span class="nc">DistributedDataParallel</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :class:`apex.parallel.DistributedDataParallel` is a module wrapper that enables</span>
<span class="sd">    easy multiprocess distributed data parallel training, similar to ``torch.nn.parallel.DistributedDataParallel``.  Parameters are broadcast across participating processes on initialization, and gradients are</span>
<span class="sd">    allreduced and averaged over processes during ``backward()``.</span>

<span class="sd">    :class:`DistributedDataParallel` is optimized for use with NCCL.  It achieves high performance by</span>
<span class="sd">    overlapping communication with computation during ``backward()`` and bucketing smaller gradient</span>
<span class="sd">    transfers to reduce the total number of transfers required.</span>

<span class="sd">    :class:`DistributedDataParallel` is designed to work with the upstream launch utility script</span>
<span class="sd">    ``torch.distributed.launch`` with ``--nproc_per_node &lt;= number of gpus per node``.</span>
<span class="sd">    When used with this launcher, :class:`DistributedDataParallel` assumes 1:1 mapping of processes to GPUs.</span>
<span class="sd">    It also assumes that your script calls ``torch.cuda.set_device(args.rank)`` before creating the model.</span>

<span class="sd">    https://github.com/NVIDIA/apex/tree/master/examples/simple/distributed shows detailed usage.</span>
<span class="sd">    https://github.com/NVIDIA/apex/tree/master/examples/imagenet shows another example</span>
<span class="sd">    that combines :class:`DistributedDataParallel` with mixed precision training.</span>

<span class="sd">    Args:</span>
<span class="sd">        module: Network definition to be run in multi-gpu/distributed mode.</span>
<span class="sd">        message_size (int, default=1e7): Minimum number of elements in a communication bucket.</span>
<span class="sd">        delay_allreduce (bool, default=False):  Delay all communication to the end of the backward pass.  This disables overlapping communication with computation.</span>
<span class="sd">        allreduce_trigger_params (list, optional, default=None):  If supplied, should contain a list of parameters drawn from the model.  Allreduces will be kicked off whenever one of these parameters receives its gradient (as opposed to when a bucket of size message_size is full).  At the end of backward(), a cleanup allreduce to catch any remaining gradients will also be performed automatically.  If allreduce_trigger_params is supplied, the message_size argument will be ignored.</span>
<span class="sd">        allreduce_always_fp32 (bool, default=False):  Convert any FP16 gradients to FP32 before allreducing.  This can improve stability for widely scaled-out runs.</span>
<span class="sd">        gradient_average (bool, default=True):  Option to toggle whether or not DDP averages the allreduced gradients over processes.  For proper scaling, the default value of True is recommended.</span>
<span class="sd">        gradient_predivide_factor (float, default=1.0):  Allows perfoming the average of gradients over processes partially before and partially after the allreduce.  Before allreduce:  ``grads.mul_(1.0/gradient_predivide_factor)``.  After allreduce:  ``grads.mul_(gradient_predivide_factor/world size)``.  This can reduce the stress on the dynamic range of FP16 allreduces for widely scaled-out runs.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If ``gradient_average=False``, the pre-allreduce division (``grads.mul_(1.0/gradient_predivide_factor)``) will still be applied, but the post-allreduce gradient averaging (``grads.mul_(gradient_predivide_factor/world size)``) will be omitted.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">module</span><span class="p">,</span>
                 <span class="n">message_size</span><span class="o">=</span><span class="mi">10000000</span><span class="p">,</span>
                 <span class="n">delay_allreduce</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">shared_param</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">allreduce_trigger_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">retain_allreduce_buffers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">allreduce_always_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">num_allreduce_streams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">allreduce_communicators</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">gradient_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">gradient_predivide_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">gradient_average_split_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">prof</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Backward/forward compatibility around</span>
        <span class="c1"># https://github.com/pytorch/pytorch/commit/540ef9b1fc5506369a48491af8a285a686689b36 and</span>
        <span class="c1"># https://github.com/pytorch/pytorch/commit/044d00516ccd6572c0d6ab6d54587155b02a3b86</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="s2">&quot;get_backend&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="s2">&quot;DistBackend&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backend_enum_holder</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">DistBackend</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backend_enum_holder</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Backend</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">_backend</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backend_enum_holder</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">dist_backend</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">warn_on_half</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend_enum_holder</span><span class="o">.</span><span class="n">GLOO</span> <span class="k">else</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prof</span> <span class="o">=</span> <span class="n">prof</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_allreduce_streams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span> <span class="o">=</span> <span class="n">num_allreduce_streams</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_communicators</span> <span class="o">=</span> <span class="n">allreduce_communicators</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_communicators</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">allreduce_communicators</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="n">num_allreduce_streams</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">allreduce_communicators</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">allreduce_communicators</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span> <span class="ow">and</span> <span class="n">delay_allreduce</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;self.allreduce_different_streams may only be used if delay_allreduce=False.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">shared_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;shared_param is no longer supported as an option.  It was misleadingly named from the start.  It turns out overlapping communication with computation should work fine with shared parameters.  If you still wish to delay communication to the end of the backward pass, use delay_allreduce=True|False instead.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">retain_allreduce_buffers</span> <span class="o">=</span> <span class="n">retain_allreduce_buffers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_always_fp32</span> <span class="o">=</span> <span class="n">allreduce_always_fp32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_average</span> <span class="o">=</span> <span class="n">gradient_average</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span> <span class="o">=</span> <span class="n">gradient_predivide_factor</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">custom_allreduce_triggers</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">allreduce_trigger_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">delay_allreduce</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Setting allreduce_trigger_params is only valid if delay_allreduce=False.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">custom_allreduce_triggers</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_trigger_params</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="nb">id</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">allreduce_trigger_params</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">delay_allreduce</span> <span class="o">=</span> <span class="n">delay_allreduce</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">message_size</span> <span class="o">=</span> <span class="n">message_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">main_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_disable_allreduce</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend_enum_holder</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">assert</span> <span class="n">param</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">,</span> <span class="s2">&quot;NCCL backend only supports model parameters to be on GPU.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">active_params</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">param_type_to_tmp_i</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;torch.cuda.HalfTensor&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                                    <span class="s2">&quot;torch.cuda.FloatTensor&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                    <span class="s2">&quot;torch.cuda.DoubleTensor&quot;</span> <span class="p">:</span> <span class="mi">2</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">multi_tensor_applier</span><span class="o">.</span><span class="n">available</span><span class="p">:</span>
            <span class="c1"># TODO:  I really need to centralize the C++ backed imports</span>
            <span class="kn">import</span> <span class="nn">amp_C</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multi_tensor_scale</span> <span class="o">=</span> <span class="n">amp_C</span><span class="o">.</span><span class="n">multi_tensor_scale</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_overflow_buf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">create_hooks</span><span class="p">()</span>

        <span class="n">flat_dist_call</span><span class="p">([</span><span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()],</span> <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="p">)</span>


    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span> <span class="ow">and</span> <span class="n">delay_allreduce</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;self.allreduce_different_streams may only be used if delay_allreduce=False.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">delay_allreduce</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">needs_refresh</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span> <span class="o">=</span> <span class="p">[]</span>


    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend_enum_holder</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s1">&#39;self.bucket_streams&#39;</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s1">&#39;self.bucket_events&#39;</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">attrs</span>

    <span class="k">def</span> <span class="nf">enable_allreduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disable_allreduce</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">disable_allreduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disable_allreduce</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Broadcast rank 0&#39;s bucket structure across all processes, and have all processes</span>
    <span class="c1"># regenerate their bucket structures to match.</span>
    <span class="k">def</span> <span class="nf">sync_bucket_structure</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Append leftover buckets</span>
        <span class="k">for</span> <span class="n">tmp_bucket</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tmp_buckets</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp_bucket</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tmp_bucket</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span><span class="p">]</span>

        <span class="n">info_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">]</span> <span class="o">+</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span> <span class="o">+</span>
                                           <span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span><span class="p">)))</span>

        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">info_tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">info</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">info_tensor</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)]</span>
        <span class="c1"># Technically, active_i_buckets&#39; work is done.  But the information is still useful to</span>
        <span class="c1"># keep around.  Therefore, refresh active_i_buckets based on rank 0 as well.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
                                 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)]</span>

        <span class="n">flattened_buckets</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="n">flat_i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">bucket_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">bucket_loc</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]):</span>
                <span class="n">param_i</span> <span class="o">=</span> <span class="n">flattened_buckets</span><span class="p">[</span><span class="n">flat_i</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">][</span><span class="n">bucket_loc</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_i</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">param_id_to_bucket</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_params</span><span class="p">[</span><span class="n">param_i</span><span class="p">])]</span> <span class="o">=</span> <span class="p">(</span><span class="n">bucket_idx</span><span class="p">,</span> <span class="n">bucket_loc</span><span class="p">)</span>
                <span class="n">flat_i</span> <span class="o">+=</span> <span class="mi">1</span>


    <span class="k">def</span> <span class="nf">create_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Fallback hook that&#39;s only called at the end of backward.</span>
        <span class="c1"># Used if you deliberately want to delay allreduces to the end, or to refresh the</span>
        <span class="c1"># bucket structure that will be used to overlap communication with computation in later</span>
        <span class="c1"># iterations.</span>
        <span class="k">def</span> <span class="nf">allreduce_params</span><span class="p">():</span>
            <span class="c1"># Bucket record refresh</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">delay_allreduce</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">needs_refresh</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">sync_bucket_structure</span><span class="p">()</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">needs_refresh</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_fallback</span><span class="p">()</span>


        <span class="k">def</span> <span class="nf">overlapping_backward_epilogue</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">stream</span><span class="p">,</span> <span class="n">event</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span><span class="p">):</span>
                <span class="n">stream</span><span class="o">.</span><span class="n">record_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>

            <span class="c1"># Sanity checks that all the buckets were kicked off</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;In epilogue, next_bucket (</span><span class="si">{}</span><span class="s2">) != num_buckets (</span><span class="si">{}</span><span class="s2">).  &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">),</span>
                                   <span class="s2">&quot;This probably indicates some buckets were not allreduced.&quot;</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">actual</span><span class="p">,</span> <span class="n">expected</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">actual</span> <span class="o">!=</span> <span class="n">expected</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Some param buckets were not allreduced.&quot;</span><span class="p">)</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">grad_accs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
                    <span class="n">param_tmp</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                    <span class="n">grad_acc</span> <span class="o">=</span> <span class="n">param_tmp</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

                    <span class="k">def</span> <span class="nf">allreduce_hook</span><span class="p">(</span><span class="o">*</span><span class="n">unused</span><span class="p">):</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prof</span><span class="p">:</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">nvtx</span><span class="o">.</span><span class="n">range_push</span><span class="p">(</span><span class="s2">&quot;allreduce_hook&quot;</span><span class="p">)</span>

                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disable_allreduce</span><span class="p">:</span>
                            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">delay_allreduce</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">needs_refresh</span><span class="p">:</span>
                                <span class="c1"># TODO:  How do we want to handle multiple backward passes between</span>
                                <span class="c1"># each forward, e.g., backward passes with retain_graph=True?</span>
                                <span class="c1"># needs_refresh and callback_queued are both vulnerable states.</span>
                                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">delay_allreduce</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">needs_refresh</span><span class="p">:</span>
                                    <span class="c1"># Use the backward pass to build the bucket structure on the fly.</span>
                                    <span class="n">active_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_id_to_active_i</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">param</span><span class="p">)]</span>

                                    <span class="c1"># Float, half, and double tensors are grouped into buckets separately.</span>
                                    <span class="n">current_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_type_to_tmp_i</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">type</span><span class="p">()]</span>

                                    <span class="bp">self</span><span class="o">.</span><span class="n">tmp_buckets</span><span class="p">[</span><span class="n">current_type</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">active_i</span><span class="p">)</span>

                                    <span class="n">ship_tmp_bucket</span> <span class="o">=</span> <span class="kc">False</span>
                                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_allreduce_triggers</span><span class="p">:</span>
                                        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_trigger_params</span><span class="p">:</span>
                                            <span class="n">ship_tmp_bucket</span> <span class="o">=</span> <span class="kc">True</span>
                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">tmp_numels</span><span class="p">[</span><span class="n">current_type</span><span class="p">]</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tmp_numels</span><span class="p">[</span><span class="n">current_type</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">message_size</span><span class="p">:</span>
                                            <span class="n">ship_tmp_bucket</span> <span class="o">=</span> <span class="kc">True</span>

                                    <span class="c1"># To consider:  If custom_allreduce_triggers are in use, ship all</span>
                                    <span class="c1"># tmp_buckets, not just tmp_buckets[current_type].</span>
                                    <span class="k">if</span> <span class="n">ship_tmp_bucket</span><span class="p">:</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tmp_buckets</span><span class="p">[</span><span class="n">current_type</span><span class="p">])</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">tmp_buckets</span><span class="p">[</span><span class="n">current_type</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">tmp_numels</span><span class="p">[</span><span class="n">current_type</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

                                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_queued</span><span class="p">:</span>
                                    <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span><span class="n">allreduce_params</span><span class="p">)</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">callback_queued</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_queued</span><span class="p">:</span>
                                    <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span><span class="n">overlapping_backward_epilogue</span><span class="p">)</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">callback_queued</span> <span class="o">=</span> <span class="kc">True</span>

                                <span class="bp">self</span><span class="o">.</span><span class="n">comm_ready_buckets</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prof</span><span class="p">:</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">nvtx</span><span class="o">.</span><span class="n">range_pop</span><span class="p">()</span>

                    <span class="n">grad_acc</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">allreduce_hook</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">grad_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_acc</span><span class="p">)</span>

                <span class="n">wrapper</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">_stream_this_bucket</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bucket_idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span><span class="p">[</span><span class="n">bucket_idx</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


    <span class="k">def</span> <span class="nf">_event_this_bucket</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bucket_idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span><span class="p">[</span><span class="n">bucket_idx</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


    <span class="k">def</span> <span class="nf">allreduce_bucket</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">force_default_stream</span><span class="p">):</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">force_default_stream</span><span class="p">:</span>
            <span class="n">bucket_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_stream</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bucket_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stream_this_bucket</span><span class="p">(</span><span class="n">bucket_idx</span><span class="p">)</span>
            <span class="n">bucket_event</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_event_this_bucket</span><span class="p">(</span><span class="n">bucket_idx</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">record_event</span><span class="p">(</span><span class="n">bucket_event</span><span class="p">)</span>
            <span class="n">bucket_stream</span><span class="o">.</span><span class="n">wait_event</span><span class="p">(</span><span class="n">bucket_event</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">bucket_stream</span><span class="p">):</span>
            <span class="c1"># self.main_stream.wait_stream(torch.cuda.current_stream())</span>
            <span class="c1"># torch.cuda.synchronize()</span>

            <span class="n">tensor_to_allreduce</span> <span class="o">=</span> <span class="n">tensor</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_always_fp32</span><span class="p">:</span>
                <span class="n">tensor_to_allreduce</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">tensor_to_allreduce</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">force_default_stream</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_to_allreduce</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_pgs</span><span class="p">[</span><span class="n">bucket_idx</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_to_allreduce</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_average</span><span class="p">:</span>
                <span class="n">tensor_to_allreduce</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_always_fp32</span> <span class="ow">and</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">tensor_to_allreduce</span><span class="p">:</span>
                <span class="n">tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tensor_to_allreduce</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">retain_allreduce_buffers</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">multi_tensor_applier</span><span class="o">.</span><span class="n">available</span><span class="p">:</span>
                    <span class="n">multi_tensor_applier</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">multi_tensor_scale</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_overflow_buf</span><span class="p">,</span>
                        <span class="p">[</span><span class="n">unflatten</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">bucket</span><span class="p">),</span> <span class="n">bucket</span><span class="p">],</span>
                        <span class="mf">1.0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">buf</span><span class="p">,</span> <span class="n">synced</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">bucket</span><span class="p">)):</span>
                        <span class="n">buf</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">synced</span><span class="p">)</span>

            <span class="c1"># I think we actually do need this here.  After allreduce_bucket returns, tensor will</span>
            <span class="c1"># eventually go out of scope and die, at which point it could otherwise be freed for</span>
            <span class="c1"># further reuse by the main stream while the allreduce/div/unflatten are underway in bucket_stream.</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">bucket_stream</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tensor</span>


    <span class="k">def</span> <span class="nf">allreduce_maybe_retain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">force_default_stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">allreduced</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_bucket</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">force_default_stream</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">retain_allreduce_buffers</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_buffers</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The backward pass is attempting to replace an already-filled &quot;</span>
                                   <span class="s2">&quot;allreduce buffer.  This is almost certainly an error.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_buffers</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">allreduced</span>
            <span class="k">for</span> <span class="n">view</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unflatten</span><span class="p">(</span><span class="n">allreduced</span><span class="p">,</span> <span class="n">bucket</span><span class="p">),</span> <span class="n">bucket</span><span class="p">):</span>
                <span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">view</span>
            <span class="c1"># for buf, synced in zip(bucket, unflatten(allreduced, bucket)):</span>
            <span class="c1">#     buf.copy_(synced)</span>


    <span class="k">def</span> <span class="nf">allreduce_fallback</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">stream</span><span class="p">,</span> <span class="n">event</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span><span class="p">):</span>
            <span class="n">stream</span><span class="o">.</span><span class="n">record_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">retain_allreduce_buffers</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>

        <span class="n">split_buckets</span> <span class="o">=</span> <span class="n">split_half_float_double</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>

        <span class="c1"># If retain_allreduce_buffers is True and delay_allreduce is False,</span>
        <span class="c1"># this will only be done during the first backward pass, ignored by the</span>
        <span class="c1"># training script, and overwritten in the next forward pass.  So it&#39;s harmless.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">retain_allreduce_buffers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_buffers</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">split_buckets</span><span class="p">))]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">split_buckets</span><span class="p">):</span>
            <span class="n">allreduced</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_maybe_retain</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">force_default_stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">comm_ready_buckets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="c1"># Need to do this in every hook for compatibility with Ruberry&#39;s streaming backward PR.</span>
        <span class="c1"># self.reduction_stream.wait_stream(torch.cuda.current_stream())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prof</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">nvtx</span><span class="o">.</span><span class="n">range_push</span><span class="p">(</span><span class="s2">&quot;comm_ready_buckets&quot;</span><span class="p">)</span>

        <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">bucket_loc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_id_to_bucket</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">param</span><span class="p">)]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">][</span><span class="n">bucket_loc</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The backward pass is attempting to replace an already-filled &quot;</span>
                               <span class="s2">&quot;bucket slot.  This is almost certainly an error.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">retain_allreduce_buffers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">][</span><span class="n">bucket_loc</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">][</span><span class="n">bucket_loc</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">bucket_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_maybe_retain</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">],</span> <span class="n">bucket_idx</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Reversing upstream&#39;s logic here, because we constructed our buckets based on</span>
                <span class="c1"># the order things were received during backward.</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">sorted_todo</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sorted_todo</span><span class="p">:</span>
                        <span class="c1"># Nothing can be reduced now</span>
                        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span><span class="p">:</span>
                            <span class="k">break</span>
                        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_maybe_retain</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">)</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;i should always be &gt;= next_bucket&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">bucket_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prof</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">nvtx</span><span class="o">.</span><span class="n">range_pop</span><span class="p">()</span>


<div class="viewcode-block" id="DistributedDataParallel.forward"><a class="viewcode-back" href="../../../parallel.html#apex.parallel.DistributedDataParallel.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prof</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">nvtx</span><span class="o">.</span><span class="n">range_push</span><span class="p">(</span><span class="s2">&quot;forward pass DDP logic&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disable_allreduce</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">delay_allreduce</span><span class="p">:</span>
                <span class="n">param_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>

                <span class="c1"># Conditions under which to refresh self.record</span>
                <span class="c1"># Forward has the authority to set needs_refresh to True, but only allreduce_params</span>
                <span class="c1"># in backward has the authority to set needs_refresh to False.</span>
                <span class="c1"># Parentheses are not necessary for correct order of operations, but make the intent clearer.</span>
                <span class="k">if</span> <span class="p">((</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_params</span><span class="p">)</span> <span class="ow">or</span>
                    <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param_list</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">active_params</span><span class="p">))</span> <span class="ow">or</span>
                    <span class="nb">any</span><span class="p">([</span><span class="n">param1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">param2</span> <span class="k">for</span> <span class="n">param1</span><span class="p">,</span> <span class="n">param2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">param_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_params</span><span class="p">)])):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">needs_refresh</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">needs_refresh</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">active_i_buckets</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">tmp_buckets</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[]]</span> <span class="c1"># [running half, float, double buckets]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">tmp_numels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">param_id_to_active_i</span> <span class="o">=</span> <span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_list</span><span class="p">)}</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">param_id_to_bucket</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bucket_pgs</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># self.buckets = [[None for _ in range(self.bucket_sizes[i])]</span>
                    <span class="c1">#                 for i in range(self.num_buckets)]</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
                                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">,</span> <span class="s2">&quot;len(buckets) = </span><span class="si">{}</span><span class="s2">, expected </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">):</span>
                            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="s2">&quot;len(buckets[</span><span class="si">{}</span><span class="s2">]) = </span><span class="si">{}</span><span class="s2">, expected </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="n">b</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">buckets</span><span class="p">[</span><span class="n">b</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>
                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bucket</span><span class="p">)):</span>
                                <span class="n">bucket</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_communicators</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_pgs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_communicators</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_communicators</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span><span class="p">)]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span><span class="p">:</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_pgs</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">bucket_pgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span><span class="p">)]</span>
                                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_pgs</span><span class="p">):</span>
                                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rank </span><span class="si">{}</span><span class="s2"> created group </span><span class="si">{}</span><span class="s2"> with backend </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                          <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span> <span class="n">i</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">bg</span><span class="p">)))</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_different_streams</span><span class="p">:</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span><span class="p">)]</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                      <span class="n">blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_allreduce_streams</span><span class="p">)]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">bucket_streams</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()]</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">bucket_events</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)]</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)]</span>
                    <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">retain_allreduce_buffers</span><span class="p">):</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_buffers</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">active_params</span> <span class="o">=</span> <span class="n">param_list</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">callback_queued</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prof</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">nvtx</span><span class="o">.</span><span class="n">range_pop</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">result</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>