

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>apex.parallel &mdash; Apex 0.1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Apex 0.1.0 documentation" href="index.html"/>
        <link rel="next" title="apex.optimizers" href="optimizers.html"/>
        <link rel="prev" title="apex.fp16_utils" href="fp16_utils.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Apex
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">AMP:  Automatic Mixed Precision</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="amp.html">apex.amp</a></li>
</ul>
<p class="caption"><span class="caption-text">FP16/Mixed Precision Utilities</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="fp16_utils.html">apex.fp16_utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="fp16_utils.html#automatic-management-of-master-params-loss-scaling">Automatic management of master params + loss scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="fp16_utils.html#manual-master-parameter-management">Manual master parameter management</a></li>
<li class="toctree-l2"><a class="reference internal" href="fp16_utils.html#custom-operations">Custom Operations</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Distributed Training</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">apex.parallel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#utility-functions">Utility functions</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Fused Optimizers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimizers.html">apex.optimizers</a></li>
</ul>
<p class="caption"><span class="caption-text">Fused Layer Norm</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="layernorm.html">apex.normalization.fused_layer_norm</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Apex</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>apex.parallel</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/parallel.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-apex.parallel">
<span id="apex-parallel"></span><h1>apex.parallel<a class="headerlink" href="#module-apex.parallel" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="apex.parallel.DistributedDataParallel">
<em class="property">class </em><code class="descclassname">apex.parallel.</code><code class="descname">DistributedDataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>message_size=10000000</em>, <em>delay_allreduce=False</em>, <em>shared_param=None</em>, <em>allreduce_trigger_params=None</em>, <em>retain_allreduce_buffers=False</em>, <em>allreduce_always_fp32=False</em>, <em>gradient_average=True</em>, <em>gradient_predivide_factor=1.0</em>, <em>gradient_average_split_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/apex/parallel/distributed.html#DistributedDataParallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#apex.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#apex.parallel.DistributedDataParallel" title="apex.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal"><span class="pre">apex.parallel.DistributedDataParallel</span></code></a> is a module wrapper that enables
easy multiprocess distributed data parallel training, similar to <code class="docutils literal"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code>.  Parameters are broadcast across participating processes on initialization, and gradients are
allreduced and averaged over processes during <code class="docutils literal"><span class="pre">backward()</span></code>.</p>
<p><a class="reference internal" href="#apex.parallel.DistributedDataParallel" title="apex.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal"><span class="pre">DistributedDataParallel</span></code></a> is optimized for use with NCCL.  It achieves high performance by
overlapping communication with computation during <code class="docutils literal"><span class="pre">backward()</span></code> and bucketing smaller gradient
transfers to reduce the total number of transfers required.</p>
<p><a class="reference internal" href="#apex.parallel.DistributedDataParallel" title="apex.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal"><span class="pre">DistributedDataParallel</span></code></a> is designed to work with the upstream launch utility script
<code class="docutils literal"><span class="pre">torch.distributed.launch</span></code> with <code class="docutils literal"><span class="pre">--nproc_per_node</span> <span class="pre">&lt;=</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">gpus</span> <span class="pre">per</span> <span class="pre">node</span></code>.
When used with this launcher, <a class="reference internal" href="#apex.parallel.DistributedDataParallel" title="apex.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal"><span class="pre">DistributedDataParallel</span></code></a> assumes 1:1 mapping of processes to GPUs.
It also assumes that your script calls <code class="docutils literal"><span class="pre">torch.cuda.set_device(args.rank)</span></code> before creating the model.</p>
<p><a class="reference external" href="https://github.com/NVIDIA/apex/tree/master/examples/distributed">https://github.com/NVIDIA/apex/tree/master/examples/distributed</a> shows detailed usage.
<a class="reference external" href="https://github.com/NVIDIA/apex/tree/master/examples/imagenet">https://github.com/NVIDIA/apex/tree/master/examples/imagenet</a> shows another example
that combines <a class="reference internal" href="#apex.parallel.DistributedDataParallel" title="apex.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal"><span class="pre">DistributedDataParallel</span></code></a> with mixed precision training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> – Network definition to be run in multi-gpu/distributed mode.</li>
<li><strong>message_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>default=1e7</em>) – Minimum number of elements in a communication bucket.</li>
<li><strong>delay_allreduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>default=False</em>) – Delay all communication to the end of the backward pass.  This disables overlapping communication with computation.</li>
<li><strong>allreduce_trigger_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – If supplied, should contain a list of parameters drawn from the model.  Allreduces will be kicked off whenever one of these parameters receives its gradient (as opposed to when a bucket of size message_size is full).  At the end of backward(), a cleanup allreduce to catch any remaining gradients will also be performed automatically.  If allreduce_trigger_params is supplied, the message_size argument will be ignored.</li>
<li><strong>allreduce_always_fp32</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>default=False</em>) – Convert any FP16 gradients to FP32 before allreducing.  This can improve stability for widely scaled-out runs.</li>
<li><strong>gradient_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>default=True</em>) – Option to toggle whether or not DDP averages the allreduced gradients over processes.  For proper scaling, the default value of True is recommended.</li>
<li><strong>gradient_predivide_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>default=1.0</em>) – Allows perfoming the average of gradients over processes partially before and partially after the allreduce.  Before allreduce:  <code class="docutils literal"><span class="pre">grads.mul_(1.0/gradient_predivide_factor)</span></code>.  After allreduce:  <code class="docutils literal"><span class="pre">grads.mul_(gradient_predivide_factor/world</span> <span class="pre">size)</span></code>.  This can reduce the stress on the dynamic range of FP16 allreduces for widely scaled-out runs.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If <code class="docutils literal"><span class="pre">gradient_average=False</span></code>, the pre-allreduce division (<code class="docutils literal"><span class="pre">grads.mul_(1.0/gradient_predivide_factor)</span></code>) will still be applied, but the post-allreduce gradient averaging (<code class="docutils literal"><span class="pre">grads.mul_(gradient_predivide_factor/world</span> <span class="pre">size)</span></code>) will be omitted.</p>
</div>
</dd></dl>

<dl class="class">
<dt id="apex.parallel.Reducer">
<em class="property">class </em><code class="descclassname">apex.parallel.</code><code class="descname">Reducer</code><span class="sig-paren">(</span><em>module_or_grads_list</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/apex/parallel/distributed.html#Reducer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#apex.parallel.Reducer" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#apex.parallel.Reducer" title="apex.parallel.Reducer"><code class="xref py py-class docutils literal"><span class="pre">apex.parallel.Reducer</span></code></a> is a simple class that helps allreduce a module’s parameters
across processes.  <a class="reference internal" href="#apex.parallel.Reducer" title="apex.parallel.Reducer"><code class="xref py py-class docutils literal"><span class="pre">Reducer</span></code></a> is intended to give the user additional control:
Unlike <a class="reference internal" href="#apex.parallel.DistributedDataParallel" title="apex.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal"><span class="pre">DistributedDataParallel</span></code></a>, <a class="reference internal" href="#apex.parallel.Reducer" title="apex.parallel.Reducer"><code class="xref py py-class docutils literal"><span class="pre">Reducer</span></code></a> will not automatically allreduce
parameters during <code class="docutils literal"><span class="pre">backward()</span></code>.
Instead, <a class="reference internal" href="#apex.parallel.Reducer" title="apex.parallel.Reducer"><code class="xref py py-class docutils literal"><span class="pre">Reducer</span></code></a> waits for the user to call <cite>&lt;reducer_instance&gt;.reduce()</cite> manually.
This enables, for example, delaying the allreduce to be carried out every
several iterations instead of every single iteration.</p>
<p>Like <a class="reference internal" href="#apex.parallel.DistributedDataParallel" title="apex.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal"><span class="pre">DistributedDataParallel</span></code></a>, <a class="reference internal" href="#apex.parallel.Reducer" title="apex.parallel.Reducer"><code class="xref py py-class docutils literal"><span class="pre">Reducer</span></code></a> averages any tensors it allreduces
over the number of participating processes.</p>
<p><a class="reference internal" href="#apex.parallel.Reducer" title="apex.parallel.Reducer"><code class="xref py py-class docutils literal"><span class="pre">Reducer</span></code></a> is designed to work with the upstream launch utility script
<code class="docutils literal"><span class="pre">torch.distributed.launch</span></code> with <code class="docutils literal"><span class="pre">--nproc_per_node</span> <span class="pre">&lt;=</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">gpus</span> <span class="pre">per</span> <span class="pre">node</span></code>.
When used with this launcher, <a class="reference internal" href="#apex.parallel.Reducer" title="apex.parallel.Reducer"><code class="xref py py-class docutils literal"><span class="pre">Reducer</span></code></a> assumes 1:1 mapping of processes to GPUs.
It also assumes that your script calls <code class="docutils literal"><span class="pre">torch.cuda.set_device(args.rank)</span></code> before creating the model.</p>
<p>main_reducer.py in <a class="reference external" href="https://github.com/NVIDIA/apex/tree/master/examples/imagenet">https://github.com/NVIDIA/apex/tree/master/examples/imagenet</a> shows example usage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>module_or_grads_list</strong> – Either a network definition (module) being run in multi-gpu/distributed mode, or an iterable of gradients to be reduced.  If a module is passed in, the Reducer constructor will sync the parameters across processes (broadcasting from rank 0) to make sure they’re all initialized with the same values.  If a list of gradients (that came from some module) is passed in, the user is responsible for manually syncing that module’s parameters at the beginning of training.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="apex.parallel.SyncBatchNorm">
<em class="property">class </em><code class="descclassname">apex.parallel.</code><code class="descname">SyncBatchNorm</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/apex/parallel/optimized_sync_batchnorm.html#SyncBatchNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#apex.parallel.SyncBatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>synchronized batch normalization module extented from <cite>torch.nn.BatchNormNd</cite>
with the added stats reduction across multiple processes.
<a class="reference internal" href="#apex.parallel.SyncBatchNorm" title="apex.parallel.SyncBatchNorm"><code class="xref py py-class docutils literal"><span class="pre">apex.parallel.SyncBatchNorm</span></code></a> is designed to work with
<cite>DistributedDataParallel</cite>.</p>
<p>When running in training mode, the layer reduces stats across all processes
to increase the effective batchsize for normalization layer. This is useful
in applications where batch size is small on a given process that would
diminish converged accuracy of the model. The model uses collective
communication package from <cite>torch.distributed</cite>.</p>
<p>When running in evaluation mode, the layer falls back to
<cite>torch.nn.functional.batch_norm</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, L)\)</span> or <span class="math">\(L\)</span> from input of size <span class="math">\((N, L)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Examples::</dt>
<dd><div class="first last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sbn</span> <span class="o">=</span> <span class="n">apex</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">sbn</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">sbn</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<div class="section" id="utility-functions">
<h2>Utility functions<a class="headerlink" href="#utility-functions" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="apex.parallel.convert_syncbn_model">
<code class="descclassname">apex.parallel.</code><code class="descname">convert_syncbn_model</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/apex/parallel.html#convert_syncbn_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#apex.parallel.convert_syncbn_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Recursively traverse module and its children to replace all
<cite>torch.nn.modules.batchnorm._BatchNorm</cite> with <cite>apex.parallel.SyncBatchNorm</cite></p>
<p>All <cite>torch.nn.BatchNorm*N*d</cite> wraps around
<cite>torch.nn.modules.batchnorm._BatchNorm</cite>, this function let you easily switch
to use sync BN.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>module</strong> – input module <cite>torch.nn.Module</cite></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Examples::</dt>
<dd><div class="first last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># model is an instance of torch.nn.Module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">apex</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sync_bn_model</span> <span class="o">=</span> <span class="n">apex</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">convert_syncbn_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="optimizers.html" class="btn btn-neutral float-right" title="apex.optimizers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="fp16_utils.html" class="btn btn-neutral" title="apex.fp16_utils" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>